CUDA_DEVICE=/dev/nvidia/0
Sat Jul  5 17:37:22 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:DB:00.0 Off |                    0 |
| N/A   37C    P0             61W /  400W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Module load: anaconda/3-2023.07-2
no change     /opt/modules/library/cpu/anaconda/3-2023.07-2/condabin/conda
no change     /opt/modules/library/cpu/anaconda/3-2023.07-2/bin/conda
no change     /opt/modules/library/cpu/anaconda/3-2023.07-2/bin/conda-env
no change     /opt/modules/library/cpu/anaconda/3-2023.07-2/bin/activate
no change     /opt/modules/library/cpu/anaconda/3-2023.07-2/bin/deactivate
no change     /opt/modules/library/cpu/anaconda/3-2023.07-2/etc/profile.d/conda.sh
no change     /opt/modules/library/cpu/anaconda/3-2023.07-2/etc/fish/conf.d/conda.fish
no change     /opt/modules/library/cpu/anaconda/3-2023.07-2/shell/condabin/Conda.psm1
no change     /opt/modules/library/cpu/anaconda/3-2023.07-2/shell/condabin/conda-hook.ps1
no change     /opt/modules/library/cpu/anaconda/3-2023.07-2/lib/python3.11/site-packages/xontrib/conda.xsh
no change     /opt/modules/library/cpu/anaconda/3-2023.07-2/etc/profile.d/conda.csh
no change     /home/aaron.dichoso/.bashrc
No action taken.
/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
CuDNN 90501
TORCH 2.7.1+cu126
CUDA 12.6
Ikou!
Generator learning rate: 2.5e-05
Discriminator learning rate: 1e-05
/scratch1/aaron.dichoso/Networks/OCENet/model/ResNet_models.py:66: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.
  self.sal_init = F.upsample(self.sal_init, size=(x.shape[2], x.shape[3]), mode='bilinear', align_corners=True)
/scratch1/aaron.dichoso/Networks/OCENet/model/ResNet_models.py:67: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.
  self.sal_ref = F.upsample(self.sal_ref, size=(x.shape[2], x.shape[3]), mode='bilinear', align_corners=True)
/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
2025-07-05 17:40:22.774607 Epoch [001/050], Step [0010/0575], COD Loss: 1.0968, OCE Loss: 0.7807
2025-07-05 17:40:48.454522 Epoch [001/050], Step [0020/0575], COD Loss: 1.0792, OCE Loss: 0.7729
2025-07-05 17:41:14.239056 Epoch [001/050], Step [0030/0575], COD Loss: 1.0334, OCE Loss: 0.7652
2025-07-05 17:41:39.917083 Epoch [001/050], Step [0040/0575], COD Loss: 0.9999, OCE Loss: 0.7588
2025-07-05 17:42:05.686819 Epoch [001/050], Step [0050/0575], COD Loss: 0.9334, OCE Loss: 0.7466
2025-07-05 17:42:31.639255 Epoch [001/050], Step [0060/0575], COD Loss: 0.9005, OCE Loss: 0.7358
2025-07-05 17:42:57.343028 Epoch [001/050], Step [0070/0575], COD Loss: 0.8892, OCE Loss: 0.7261
2025-07-05 17:43:23.071904 Epoch [001/050], Step [0080/0575], COD Loss: 0.8920, OCE Loss: 0.7166
2025-07-05 17:43:48.815548 Epoch [001/050], Step [0090/0575], COD Loss: 0.8975, OCE Loss: 0.7073
2025-07-05 17:44:14.571372 Epoch [001/050], Step [0100/0575], COD Loss: 0.8333, OCE Loss: 0.6984
2025-07-05 17:44:40.360888 Epoch [001/050], Step [0110/0575], COD Loss: 0.8159, OCE Loss: 0.6899
2025-07-05 17:45:06.300363 Epoch [001/050], Step [0120/0575], COD Loss: 0.8042, OCE Loss: 0.6815
2025-07-05 17:45:32.018511 Epoch [001/050], Step [0130/0575], COD Loss: 0.7647, OCE Loss: 0.6733
2025-07-05 17:45:57.846697 Epoch [001/050], Step [0140/0575], COD Loss: 0.7550, OCE Loss: 0.6650
2025-07-05 17:46:23.633019 Epoch [001/050], Step [0150/0575], COD Loss: 0.7128, OCE Loss: 0.6566
2025-07-05 17:46:49.419887 Epoch [001/050], Step [0160/0575], COD Loss: 0.6300, OCE Loss: 0.6484
2025-07-05 17:47:15.242884 Epoch [001/050], Step [0170/0575], COD Loss: 0.5768, OCE Loss: 0.6391
2025-07-05 17:47:41.168015 Epoch [001/050], Step [0180/0575], COD Loss: 0.5708, OCE Loss: 0.6296
2025-07-05 17:48:07.024513 Epoch [001/050], Step [0190/0575], COD Loss: 0.6315, OCE Loss: 0.6184
2025-07-05 17:48:32.841932 Epoch [001/050], Step [0200/0575], COD Loss: 0.7045, OCE Loss: 0.6076
2025-07-05 17:48:58.651354 Epoch [001/050], Step [0210/0575], COD Loss: 0.7714, OCE Loss: 0.5978
2025-07-05 17:49:24.463887 Epoch [001/050], Step [0220/0575], COD Loss: 0.8036, OCE Loss: 0.5883
2025-07-05 17:49:50.335227 Epoch [001/050], Step [0230/0575], COD Loss: 0.7669, OCE Loss: 0.5818
2025-07-05 17:50:16.214041 Epoch [001/050], Step [0240/0575], COD Loss: 0.7488, OCE Loss: 0.5748
2025-07-05 17:50:42.179645 Epoch [001/050], Step [0250/0575], COD Loss: 0.7486, OCE Loss: 0.5677
2025-07-05 17:51:08.094320 Epoch [001/050], Step [0260/0575], COD Loss: 0.7510, OCE Loss: 0.5613
2025-07-05 17:51:33.981911 Epoch [001/050], Step [0270/0575], COD Loss: 0.7294, OCE Loss: 0.5546
2025-07-05 17:51:59.887271 Epoch [001/050], Step [0280/0575], COD Loss: 0.7558, OCE Loss: 0.5499
2025-07-05 17:52:25.767473 Epoch [001/050], Step [0290/0575], COD Loss: 0.7698, OCE Loss: 0.5441
2025-07-05 17:52:51.663709 Epoch [001/050], Step [0300/0575], COD Loss: 0.7818, OCE Loss: 0.5399
2025-07-05 17:53:17.748053 Epoch [001/050], Step [0310/0575], COD Loss: 0.7581, OCE Loss: 0.5340
2025-07-05 17:53:43.724226 Epoch [001/050], Step [0320/0575], COD Loss: 0.6784, OCE Loss: 0.5245
2025-07-05 17:54:09.675596 Epoch [001/050], Step [0330/0575], COD Loss: 0.6141, OCE Loss: 0.5164
2025-07-05 17:54:35.743998 Epoch [001/050], Step [0340/0575], COD Loss: 0.5512, OCE Loss: 0.5102
2025-07-05 17:55:01.668867 Epoch [001/050], Step [0350/0575], COD Loss: 0.5345, OCE Loss: 0.5036
2025-07-05 17:55:27.665083 Epoch [001/050], Step [0360/0575], COD Loss: 0.5086, OCE Loss: 0.4982
2025-07-05 17:55:53.788349 Epoch [001/050], Step [0370/0575], COD Loss: 0.4825, OCE Loss: 0.4929
2025-07-05 17:56:19.746360 Epoch [001/050], Step [0380/0575], COD Loss: 0.4588, OCE Loss: 0.4842
2025-07-05 17:56:45.806399 Epoch [001/050], Step [0390/0575], COD Loss: 0.4440, OCE Loss: 0.4786
2025-07-05 17:57:11.845640 Epoch [001/050], Step [0400/0575], COD Loss: 0.4696, OCE Loss: 0.4748
2025-07-05 17:57:37.873215 Epoch [001/050], Step [0410/0575], COD Loss: 0.5057, OCE Loss: 0.4686
2025-07-05 17:58:03.972486 Epoch [001/050], Step [0420/0575], COD Loss: 0.4836, OCE Loss: 0.4610
2025-07-05 17:58:29.997925 Epoch [001/050], Step [0430/0575], COD Loss: 0.4625, OCE Loss: 0.4523
2025-07-05 17:58:56.279551 Epoch [001/050], Step [0440/0575], COD Loss: 0.4122, OCE Loss: 0.4437
2025-07-05 17:59:22.350049 Epoch [001/050], Step [0450/0575], COD Loss: 0.3491, OCE Loss: 0.4388
2025-07-05 17:59:48.432732 Epoch [001/050], Step [0460/0575], COD Loss: 0.3395, OCE Loss: 0.4335
2025-07-05 18:00:14.580056 Epoch [001/050], Step [0470/0575], COD Loss: 0.3854, OCE Loss: 0.4273
2025-07-05 18:00:40.661525 Epoch [001/050], Step [0480/0575], COD Loss: 0.4237, OCE Loss: 0.4241
2025-07-05 18:01:06.790766 Epoch [001/050], Step [0490/0575], COD Loss: 0.4642, OCE Loss: 0.4187
2025-07-05 18:01:33.117853 Epoch [001/050], Step [0500/0575], COD Loss: 0.5150, OCE Loss: 0.4142
2025-07-05 18:01:59.162429 Epoch [001/050], Step [0510/0575], COD Loss: 0.5024, OCE Loss: 0.4088
2025-07-05 18:02:25.253961 Epoch [001/050], Step [0520/0575], COD Loss: 0.4885, OCE Loss: 0.4007
2025-07-05 18:02:51.345592 Epoch [001/050], Step [0530/0575], COD Loss: 0.4688, OCE Loss: 0.3923
2025-07-05 18:03:17.427382 Epoch [001/050], Step [0540/0575], COD Loss: 0.4471, OCE Loss: 0.3839
2025-07-05 18:03:43.571547 Epoch [001/050], Step [0550/0575], COD Loss: 0.4398, OCE Loss: 0.3779
2025-07-05 18:04:09.503106 Epoch [001/050], Step [0560/0575], COD Loss: 0.4401, OCE Loss: 0.3701
2025-07-05 18:04:35.358700 Epoch [001/050], Step [0570/0575], COD Loss: 0.4506, OCE Loss: 0.3635
2025-07-05 18:04:48.999685 Epoch [001/050], Step [0575/0575], COD Loss: 0.4610, OCE Loss: 0.3605
Generator learning rate: 2.5e-05
Discriminator learning rate: 1e-05
2025-07-05 18:05:19.577921 Epoch [002/050], Step [0010/0575], COD Loss: 0.9039, OCE Loss: 0.3548
2025-07-05 18:05:45.629282 Epoch [002/050], Step [0020/0575], COD Loss: 0.8535, OCE Loss: 0.3558
2025-07-05 18:06:11.647096 Epoch [002/050], Step [0030/0575], COD Loss: 0.7797, OCE Loss: 0.3532
2025-07-05 18:06:37.555130 Epoch [002/050], Step [0040/0575], COD Loss: 0.7274, OCE Loss: 0.3489
2025-07-05 18:07:03.656712 Epoch [002/050], Step [0050/0575], COD Loss: 0.6318, OCE Loss: 0.3431
2025-07-05 18:07:29.564610 Epoch [002/050], Step [0060/0575], COD Loss: 0.5927, OCE Loss: 0.3353
2025-07-05 18:07:55.551893 Epoch [002/050], Step [0070/0575], COD Loss: 0.5716, OCE Loss: 0.3272
2025-07-05 18:08:21.499201 Epoch [002/050], Step [0080/0575], COD Loss: 0.5742, OCE Loss: 0.3289
2025-07-05 18:08:47.532085 Epoch [002/050], Step [0090/0575], COD Loss: 0.5804, OCE Loss: 0.3299
2025-07-05 18:09:13.607363 Epoch [002/050], Step [0100/0575], COD Loss: 0.5215, OCE Loss: 0.3292
2025-07-05 18:09:39.608578 Epoch [002/050], Step [0110/0575], COD Loss: 0.5550, OCE Loss: 0.3293
2025-07-05 18:10:05.836074 Epoch [002/050], Step [0120/0575], COD Loss: 0.5870, OCE Loss: 0.3169
2025-07-05 18:10:31.849900 Epoch [002/050], Step [0130/0575], COD Loss: 0.5813, OCE Loss: 0.3053
2025-07-05 18:10:57.934772 Epoch [002/050], Step [0140/0575], COD Loss: 0.5997, OCE Loss: 0.2961
2025-07-05 18:11:24.050609 Epoch [002/050], Step [0150/0575], COD Loss: 0.5758, OCE Loss: 0.2862
2025-07-05 18:11:50.107907 Epoch [002/050], Step [0160/0575], COD Loss: 0.5069, OCE Loss: 0.2803
2025-07-05 18:12:16.155256 Epoch [002/050], Step [0170/0575], COD Loss: 0.4786, OCE Loss: 0.2740
2025-07-05 18:12:42.342570 Epoch [002/050], Step [0180/0575], COD Loss: 0.4872, OCE Loss: 0.2711
2025-07-05 18:13:08.443446 Epoch [002/050], Step [0190/0575], COD Loss: 0.5473, OCE Loss: 0.2712
2025-07-05 18:13:34.667465 Epoch [002/050], Step [0200/0575], COD Loss: 0.6251, OCE Loss: 0.2711
2025-07-05 18:14:00.804317 Epoch [002/050], Step [0210/0575], COD Loss: 0.6919, OCE Loss: 0.2702
2025-07-05 18:14:27.312512 Epoch [002/050], Step [0220/0575], COD Loss: 0.7267, OCE Loss: 0.2651
2025-07-05 18:14:53.352824 Epoch [002/050], Step [0230/0575], COD Loss: 0.6935, OCE Loss: 0.2580
2025-07-05 18:15:19.555626 Epoch [002/050], Step [0240/0575], COD Loss: 0.6770, OCE Loss: 0.2512
2025-07-05 18:15:45.593383 Epoch [002/050], Step [0250/0575], COD Loss: 0.6758, OCE Loss: 0.2469
2025-07-05 18:16:11.677527 Epoch [002/050], Step [0260/0575], COD Loss: 0.6821, OCE Loss: 0.2437
2025-07-05 18:16:37.701997 Epoch [002/050], Step [0270/0575], COD Loss: 0.6613, OCE Loss: 0.2420
2025-07-05 18:17:03.812314 Epoch [002/050], Step [0280/0575], COD Loss: 0.6830, OCE Loss: 0.2445
2025-07-05 18:17:29.946962 Epoch [002/050], Step [0290/0575], COD Loss: 0.6944, OCE Loss: 0.2450
2025-07-05 18:17:56.075932 Epoch [002/050], Step [0300/0575], COD Loss: 0.7007, OCE Loss: 0.2496
2025-07-05 18:18:22.349140 Epoch [002/050], Step [0310/0575], COD Loss: 0.6732, OCE Loss: 0.2438
2025-07-05 18:18:48.471230 Epoch [002/050], Step [0320/0575], COD Loss: 0.6017, OCE Loss: 0.2338
2025-07-05 18:19:14.546517 Epoch [002/050], Step [0330/0575], COD Loss: 0.5436, OCE Loss: 0.2248
2025-07-05 18:19:40.638715 Epoch [002/050], Step [0340/0575], COD Loss: 0.4922, OCE Loss: 0.2153
2025-07-05 18:20:06.732559 Epoch [002/050], Step [0350/0575], COD Loss: 0.4829, OCE Loss: 0.2131
2025-07-05 18:20:32.793815 Epoch [002/050], Step [0360/0575], COD Loss: 0.4614, OCE Loss: 0.2107
2025-07-05 18:20:58.945739 Epoch [002/050], Step [0370/0575], COD Loss: 0.4384, OCE Loss: 0.2085
2025-07-05 18:21:25.059067 Epoch [002/050], Step [0380/0575], COD Loss: 0.4168, OCE Loss: 0.2028
2025-07-05 18:21:51.202229 Epoch [002/050], Step [0390/0575], COD Loss: 0.4031, OCE Loss: 0.2005
2025-07-05 18:22:17.306634 Epoch [002/050], Step [0400/0575], COD Loss: 0.4177, OCE Loss: 0.1982
2025-07-05 18:22:43.472951 Epoch [002/050], Step [0410/0575], COD Loss: 0.4373, OCE Loss: 0.1938
2025-07-05 18:23:09.591394 Epoch [002/050], Step [0420/0575], COD Loss: 0.4085, OCE Loss: 0.1896
2025-07-05 18:23:35.806108 Epoch [002/050], Step [0430/0575], COD Loss: 0.3798, OCE Loss: 0.1832
2025-07-05 18:24:01.934705 Epoch [002/050], Step [0440/0575], COD Loss: 0.3444, OCE Loss: 0.1782
2025-07-05 18:24:28.051518 Epoch [002/050], Step [0450/0575], COD Loss: 0.2995, OCE Loss: 0.1738
2025-07-05 18:24:54.139665 Epoch [002/050], Step [0460/0575], COD Loss: 0.2951, OCE Loss: 0.1706
2025-07-05 18:25:20.321909 Epoch [002/050], Step [0470/0575], COD Loss: 0.3376, OCE Loss: 0.1687
2025-07-05 18:25:46.449264 Epoch [002/050], Step [0480/0575], COD Loss: 0.3639, OCE Loss: 0.1687
2025-07-05 18:26:12.554659 Epoch [002/050], Step [0490/0575], COD Loss: 0.3987, OCE Loss: 0.1694
2025-07-05 18:26:38.814855 Epoch [002/050], Step [0500/0575], COD Loss: 0.4448, OCE Loss: 0.1688
2025-07-05 18:27:04.952402 Epoch [002/050], Step [0510/0575], COD Loss: 0.4418, OCE Loss: 0.1685
2025-07-05 18:27:31.170431 Epoch [002/050], Step [0520/0575], COD Loss: 0.4347, OCE Loss: 0.1652
2025-07-05 18:27:57.313376 Epoch [002/050], Step [0530/0575], COD Loss: 0.4214, OCE Loss: 0.1626
2025-07-05 18:28:23.519612 Epoch [002/050], Step [0540/0575], COD Loss: 0.4055, OCE Loss: 0.1604
2025-07-05 18:28:49.743020 Epoch [002/050], Step [0550/0575], COD Loss: 0.3975, OCE Loss: 0.1577
2025-07-05 18:29:15.821558 Epoch [002/050], Step [0560/0575], COD Loss: 0.3942, OCE Loss: 0.1554
2025-07-05 18:29:42.039292 Epoch [002/050], Step [0570/0575], COD Loss: 0.4053, OCE Loss: 0.1538
2025-07-05 18:29:54.884367 Epoch [002/050], Step [0575/0575], COD Loss: 0.4142, OCE Loss: 0.1532
Generator learning rate: 2.5e-05
Discriminator learning rate: 1e-05
