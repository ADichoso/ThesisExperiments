CUDA_DEVICE=/dev/nvidia/0
Sun Jun 22 19:43:13 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             59W /  400W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Module load: anaconda/3-2023.07-2
no change     /opt/modules/library/cpu/anaconda/3-2023.07-2/condabin/conda
no change     /opt/modules/library/cpu/anaconda/3-2023.07-2/bin/conda
no change     /opt/modules/library/cpu/anaconda/3-2023.07-2/bin/conda-env
no change     /opt/modules/library/cpu/anaconda/3-2023.07-2/bin/activate
no change     /opt/modules/library/cpu/anaconda/3-2023.07-2/bin/deactivate
no change     /opt/modules/library/cpu/anaconda/3-2023.07-2/etc/profile.d/conda.sh
no change     /opt/modules/library/cpu/anaconda/3-2023.07-2/etc/fish/conf.d/conda.fish
no change     /opt/modules/library/cpu/anaconda/3-2023.07-2/shell/condabin/Conda.psm1
no change     /opt/modules/library/cpu/anaconda/3-2023.07-2/shell/condabin/conda-hook.ps1
no change     /opt/modules/library/cpu/anaconda/3-2023.07-2/lib/python3.11/site-packages/xontrib/conda.xsh
no change     /opt/modules/library/cpu/anaconda/3-2023.07-2/etc/profile.d/conda.csh
no change     /home/aaron.dichoso/.bashrc
No action taken.
0
/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W622 19:43:33.878109101 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Ikou!
/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
Generator learning rate: 2.5e-05
Discriminator learning rate: 1e-05
/scratch1/aaron.dichoso/OCENet/model/ResNet_models.py:66: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.
  self.sal_init = F.upsample(self.sal_init, size=(x.shape[2], x.shape[3]), mode='bilinear', align_corners=True)
/scratch1/aaron.dichoso/OCENet/model/ResNet_models.py:67: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.
  self.sal_ref = F.upsample(self.sal_ref, size=(x.shape[2], x.shape[3]), mode='bilinear', align_corners=True)
/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torch/autograd/graph.py:824: UserWarning: Error detected in CudnnBatchNormBackward0. Traceback of forward call that caused the error:
  File "/scratch1/aaron.dichoso/OCENet/train.py", line 172, in <module>
    train()
  File "/scratch1/aaron.dichoso/OCENet/train.py", line 119, in train
    confi_init_pred = OCE_Net.forward(post_init)
  File "/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torch/nn/parallel/distributed.py", line 1637, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
   File "/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torch/nn/parallel/distributed.py", line 1464, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch1/aaron.dichoso/OCENet/model/ResNet_models.py", line 53, in forward
    out = self.up_block_1(out, block_1)
  File "/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch1/aaron.dichoso/OCENet/model/ResNet.py", line 261, in forward
    out = self.activation(self.bn2_2(self.conv2_2(out)))
  File "/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torch/nn/modules/batchnorm.py", line 799, in forward
    return F.batch_norm(
  File "/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torch/nn/functional.py", line 2822, in batch_norm
    return torch.batch_norm(
 (Triggered internally at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:122.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]: Traceback (most recent call last):
[rank0]:   File "/scratch1/aaron.dichoso/OCENet/train.py", line 172, in <module>
[rank0]:     train()
[rank0]:     ~~~~~^^
[rank0]:   File "/scratch1/aaron.dichoso/OCENet/train.py", line 128, in train
[rank0]:     OCE_loss.backward()
[rank0]:     ~~~~~~~~~~~~~~~~~^^
[rank0]:   File "/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torch/_tensor.py", line 648, in backward
[rank0]:     torch.autograd.backward(
[rank0]:     ~~~~~~~~~~~~~~~~~~~~~~~^
[rank0]:         self, gradient, retain_graph, create_graph, inputs=inputs
[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:     )
[rank0]:     ^
[rank0]:   File "/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torch/autograd/__init__.py", line 353, in backward
[rank0]:     _engine_run_backward(
[rank0]:     ~~~~~~~~~~~~~~~~~~~~^
[rank0]:         tensors,
[rank0]:         ^^^^^^^^
[rank0]:     ...<5 lines>...
[rank0]:         accumulate_grad=True,
[rank0]:         ^^^^^^^^^^^^^^^^^^^^^
[rank0]:     )
[rank0]:     ^
[rank0]:   File "/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:         t_outputs, *args, **kwargs
[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:     )  # Calls into the C++ engine to run the backward pass
[rank0]:     ^
[rank0]: RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [16]] is at version 3; expected version 2 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
[rank0]:[W622 19:45:33.253029830 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
E0622 19:45:34.909000 3835957 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 3835973) of binary: /home/aaron.dichoso/.conda/envs/OCETraining/bin/python
Traceback (most recent call last):
  File "/home/aaron.dichoso/.conda/envs/OCETraining/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ~~~~^^
  File "/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torch/distributed/run.py", line 892, in main
    run(args)
    ~~~^^^^^^
  File "/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
    ~~~~~~~~~~~~~~~
        config=config,
        ~~~~~~~~~~~~~~
        entrypoint=cmd,
        ~~~~~~~~~~~~~~~
    )(*cmd_args)
    ~^^^^^^^^^^^
  File "/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/aaron.dichoso/.conda/envs/OCETraining/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
    ...<2 lines>...
    )
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
OCENet/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-06-22_19:45:34
  host      : saliksik-gpu-09.coare.local
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3835973)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: saliksik-gpu-09: task 0: Exited with exit code 1
